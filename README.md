# Awesome_KnowledgeDistillation
这里收集了一些关于知识蒸馏的介绍和研究现状。

欢迎 PR！如果对你有帮助，请三连支持👍！

### DRL 基础

- [大神总结 | 强化学习线路图](https://mp.weixin.qq.com/s/E2va_w2Lh_x3n_1XnOY0ZA)

### KD 概述

- [Knowledge Distillation（知识蒸馏）Review--20篇paper回顾](https://zhuanlan.zhihu.com/p/160206075)
- [知识蒸馏 | 模型压缩利器_良心总结](https://zhuanlan.zhihu.com/p/138210881)
- Knowledge Distillation: A Survey [[paper]](https://arxiv.org/pdf/2006.05525.pdf)
- Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks [[paper]](https://arxiv.org/pdf/2004.05937.pdf)

### KD 方法

- Distilling the Knowledge in a Neural Network [[paper]](https://arxiv.org/pdf/1503.02531.pdf)
- Deep Mutual Learning [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf)
- PoPS: Policy Pruning and Shrinking for Deep Reinforcement Learning [[paper]](https://arxiv.org/pdf/2001.05012.pdf)

